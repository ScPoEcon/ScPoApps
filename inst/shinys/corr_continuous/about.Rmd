---
title: "corr_continuous"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, eval=FALSE}
library(ScPoApps)
launchApp("corr_continuous")
```

This app let's you visualize what the correlation between two random variables $X$ and $Y$ "looks" like.

Here, each of the 300 $\{(X_i, Y_i)\}$ pairs are simulated such that the correlation between $X$ and $Y$ is exactly that which is supplied by the user using the app slider.

We then plot $Y$ against $X$ (this is called a bivariate plot, or scatterplot since the data is vizualized as a scatter of points) and observe how different correlations result in different plots.

The correlation coefficient between $X$ and $Y$ is typically denoted as $r_{xy}$.

### A few things to note

+ **When $r_{xy} > 0$**, the cloud of points concentrates along an upward-sloping line: *higher-than-average* values of $X$ are generally associated with *higher-than-average* values of $Y$. We say that $X$ and $Y$ are **positively correlated**.

+ **When $r_{xy} < 0$**, the cloud of points concentrates along an downward-sloping line: *higher-than-average* values of $X$ are generally associated with *lower-than-average* values of $Y$. We say that $X$ and $Y$ are **negatively correlated**.

+ At the extreme end of this, when **$r_{xy} = 1 \; \text{or} \; -1$**, then the data lies on a straight line meaning that $X$ and $Y$ are perfectly linearly associated, or **perfectly correlated**. This means that the value of any of them can be perfectly predicted from the other. An example of two perfectly correlated variables is temperature as measured in Celcius, and as measured in Fahrenheit.

+ Finally, when **$r_{xy} = 0$**, there is absolutely no sort of (linear) association between the two variables. Generally speaking (we'll see counter-examples of this later), nothing can be said for the value of $Y$ from observing a given value of $X$. $X$ and $Y$ are said to be **uncorrelated** or **linearly independent**.


### Math Appendix (optional)

For any two random variables $X$ and $Y$ with means $\mu_x$ and $\mu_y$ and variances $\sigma_x^2$ and $\sigma_y^2$. Their **covariance** is given by:

$$Cov(X, Y) = \mathbb{E}[(X-\mu_x)(Y-\mu_y)]$$ which can be rewritten as:

$$Cov(X, Y) = \mathbb{E}[XY] - \mathbb{E}[X] \mathbb{E}[Y] = \mathbb{E}[XY] - \mu_x\mu_y$$

(Note that $Cov(X, X) = Var(X)$)

The **correlation** coefficient between $X$ and $Y$ is simply their covariance divided by the product of their standard deviation:

$$r_{xy} = \frac{Cov(X, Y)}{\sigma_x \sigma_y} = \frac{\mathbb{E}[(X-\mu_x)(Y-\mu_y)]}{\sigma_x \sigma_y}$$

In practice, correlation can be computed for any sample of size N with the following formula:

$$\hat{r}_{xy} =  \frac{\sum_{i = 1}^{N}(X_i - \bar{X})(Y_i - \bar{Y})}{s_x s_y}$$

Where $\bar{X}$ and $\bar{Y}$ are sample means and $s_x$ and $s_y$ are sample standard deviations.

(or more simply by calling `cor(X, Y)` in R !)

---
title: "reg_simple"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, eval=FALSE}
library(ScPoApps)
launchApp("reg_simple")
```

This app is a very basic, visual introduction to the concept of a type of *linear regression* known as **Ordinary Least Squares** (or OLS).

When studying the association between two (or more) variables, scientists are often looking for a relatively simple model to describe such relationship. One widely used such model is a simple line, also called the *regression line* or *line of best-fit*.

As you know from high-school maths, the equation of a line in 2 dimentions can be written as:

$$y = b_0 + b_1 x$$

Where $b_0$ (the intercept) and $b_1$ (the slope) are fixed coefficients.

The goal of OLS is to determine values for $b_0$ and $b_1$ such that the resulting line is the *best possible line* to predict the dependent variable $y$ given the independent variable $x$. In that sense, it is the line that "best fits" the observed data collected in a sample.

This notion of "best fit" is encapsulated by the minimizing of the total sum of squared errors (SSR) which is defined as follows:

$$SSR = \sum_{i = 1}^{N}(\text{observed value of }y_i - \text{predicted value of }y_i)^2$$

We usually write our predicted value of $y$ as $\hat{y}$ ("y-hat") and thus the equation becomes:

$$SSR = \sum_{i = 1}^{N}(y_i - \hat{y_i})^2$$


(Note that, by definition, $\hat{y_i} = b_0 + b_1 x_i$, and so:

$$SSR = \sum_{i = 1}^{N}(y_i - b_0 - b_1 x_i)^2$$




### Observe

+ You have access to a sample of 20 observation of x-y pairs which are plotted.
+ Using the sliders, move the regression line until you find the line of best-fit (it will turn green when you find it)
+ The red squares are the *squared* errors at each point, the sum of which (SSR) you would like to minimize. Thus, you will need to balance between 20 different squared errors, trying to make them all simulteanously as small as possible.
+ Once you have found the best-fit line, look at the line's intercept and slope coefficient. What does it mean that the slope coefficient is positive?
+ Can you find an example of when you could observe such data in real life?
+ Bonus: if we instead plotted $x$ against $y$ (i.e. switched the axes), how would your best-fit line change?

